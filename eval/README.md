# 🧪 Evaluation

We provide evaluation scripts to reproduce the results of **Skywork R1V3**.

Most benchmarks can be evaluated using **[VLMEvalKit](https://github.com/open-compass/VLMEvalKit)**, except for [EMMA-mini](https://github.com/EMMA-Bench/EMMA) and [MMK12](https://github.com/ModalMinds/MM-EUREKA).

---

### Environment Setup

```bash
bash ./eval/vlmevalkit/build_env.sh
```
### Set OpenAI API Key and Base URL
Create or edit the `.env` file in `vlmevalkit/` and add the following:

```dotenv
OPENAI_API_KEY=your_api_key_here
OPENAI_API_BASE=https://your_base_url_here
```
# 🚀 Evaluation Steps
**Step 1: Launch the Model**

Start tmux and deploy the model
```bash
tmux
export TORCH_CUDA_ARCH_LIST="8.9+PTX"
bash ./vlmevalkit/eval_shell/launch_vlm.sh
```
**Step 2: Run Evaluation Scripts**

Evaluate on supported benchmarks:
```bash
bash ./vlmevalkit/eval_shell/run_eval.sh
```
> ⚠️ **Note:** Some benchmarks (e.g., `MMMU`) require post-processing to adjust results using rule-based scripts.


# 📌 Additional Notes
- Ensure the model is properly loaded before running evaluation.
- All results will be saved under the `outputs/` directory.
- For MMK12 and EMMA-mini, please refer to their individual README files for evaluation instructions.

