#!/bin/bash
# Benchmark evaluation script for r1v4-deepresearch model
# This script runs inference and then judges the results for multiple benchmarks

set -e  # Exit on error

# =============================================================================
# Benchmark Download and Preparation Guide
# =============================================================================
#
# Download images and jsonl of each benchmark and place them in the corresponding directory
#
# For example:
#   browsecomp-vl/images/     # for images
#   browsecomp-vl/input.jsonl # input data
#
#   fvqa-test/images/         # for images
#   fvqa-test/input.jsonl     # input data
#
#   mm-search/images/         # for images
#   mm-search/input.jsonl     # input data
#
# The schema of input.jsonl should be:
# {
#     "image": "the path of the image",
#     "question": "the question",
#     "answer": "the answer"
# }
#
# Note: The official schema may be different, you need to check the official 
# schema and modify it to the schema above.
#
# Download addresses of each benchmark:
#
# - browsecomp-vl: https://github.com/Alibaba-NLP/DeepResearch/tree/main/WebAgent/WebWatcher/benchmark
#   (level1 and level2)
#
# - fvqa-test: https://huggingface.co/datasets/lmms-lab/FVQA
#   (test subset!)
#
# - mm-search: https://huggingface.co/datasets/CaraJ/MMSearch
#   (end2end subset!)
#
# =============================================================================

# =============================================================================
# Configuration - Please fill in your benchmark paths
# =============================================================================

# TODO: Fill in the paths to your benchmark directories
BENCHMARK_PATHS=(
    ""  # TODO: e.g., "/path/to/browsecomp-vl/input.jsonl"
    ""  # TODO: e.g., "/path/to/fvqa-test/input.jsonl"
    ""  # TODO: e.g., "/path/to/mm-search/input.jsonl"
)

# =============================================================================
# Expected directory structure after preparation:
# 
# browsecomp-vl/
#   ├── images/              # Image files
#   ├── input.jsonl          # Input data (user prepared)
#   ├── output.jsonl         # Generated by infer.py
#   ├── output_judged.jsonl  # Generated by o3-mini-judge.py
#   └── output_metrics.txt   # Generated by o3-mini-judge.py
# 
# fvqa-test/
#   ├── images/              # Image files
#   ├── input.jsonl          # Input data (user prepared)
#   ├── output.jsonl         # Generated by infer.py
#   ├── output_judged.jsonl  # Generated by o3-mini-judge.py
#   └── output_metrics.txt   # Generated by o3-mini-judge.py
# 
# mm-search/
#   ├── images/              # Image files
#   ├── input.jsonl          # Input data (user prepared)
#   ├── output.jsonl         # Generated by infer.py
#   ├── output_judged.jsonl  # Generated by o3-mini-judge.py
#   └── output_metrics.txt   # Generated by o3-mini-judge.py
# =============================================================================

# Script directory
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"

# Python scripts
INFER_SCRIPT="${SCRIPT_DIR}/infer.py"
JUDGE_SCRIPT="${SCRIPT_DIR}/o3-mini-judge.py"

# Parameters
MAX_TOKENS=20480
TEMPERATURE=0.2
ANSWER_COLUMN="r1v4_deepresearch_output"

# =============================================================================
# Functions
# =============================================================================

check_file_exists() {
    if [ ! -f "$1" ]; then
        echo "Error: File not found: $1"
        exit 1
    fi
}

process_benchmark() {
    local input_file="$1"
    local benchmark_name=$(basename "$(dirname "$input_file")")
    
    echo "=========================================="
    echo "Processing benchmark: ${benchmark_name}"
    echo "Input file: ${input_file}"
    echo "=========================================="
    
    # Check if input file exists
    check_file_exists "${input_file}"
    
    # Get directory and base name
    local input_dir=$(dirname "${input_file}")
    local output_file="${input_dir}/output.jsonl"
    local judged_file="${input_dir}/output_judged.jsonl"
    local metrics_file="${input_dir}/output_metrics.txt"
    
    # Step 1: Run inference
    echo ""
    echo "[Step 1/2] Running inference..."
    echo "Command: python ${INFER_SCRIPT} --input ${input_file} --output ${output_file} --max-tokens ${MAX_TOKENS} --temperature ${TEMPERATURE}"
    echo ""
    
    python "${INFER_SCRIPT}" \
        --input "${input_file}" \
        --output "${output_file}" \
        --max-tokens ${MAX_TOKENS} \
        --temperature ${TEMPERATURE}
    
    echo ""
    echo "✓ Inference completed. Output: ${output_file}"
    echo ""
    
    # Check if output file was created
    check_file_exists "${output_file}"
    
    # Step 2: Run judge
    echo ""
    echo "[Step 2/2] Running judge..."
    echo "Command: python ${JUDGE_SCRIPT} --input_jsonl ${output_file} --answer_column ${ANSWER_COLUMN}"
    echo ""
    
    python "${JUDGE_SCRIPT}" \
        --input_jsonl "${output_file}" \
        --answer_column "${ANSWER_COLUMN}"
    
    echo ""
    echo "✓ Judging completed. Output: ${judged_file}"
    echo "✓ Metrics saved to: ${metrics_file}"
    echo ""
    
    # Display metrics if file exists
    if [ -f "${metrics_file}" ]; then
        echo "--- Metrics for ${benchmark_name} ---"
        cat "${metrics_file}"
        echo ""
    fi
    
    echo "=========================================="
    echo "Benchmark ${benchmark_name} completed!"
    echo "=========================================="
    echo ""
}

# =============================================================================
# Main execution
# =============================================================================

echo "=============================================="
echo "R1V4 Benchmark Evaluation Pipeline"
echo "=============================================="
echo ""

# Check if benchmark paths are filled
all_empty=true
for path in "${BENCHMARK_PATHS[@]}"; do
    if [ -n "$path" ]; then
        all_empty=false
        break
    fi
done

if [ "$all_empty" = true ]; then
    echo "Error: Please fill in at least one benchmark path in BENCHMARK_PATHS"
    echo "Edit this script and set the paths in the Configuration section"
    exit 1
fi

# Check if Python scripts exist
check_file_exists "${INFER_SCRIPT}"
check_file_exists "${JUDGE_SCRIPT}"

# Process each benchmark
total_benchmarks=0
for input_file in "${BENCHMARK_PATHS[@]}"; do
    if [ -n "$input_file" ]; then
        ((total_benchmarks++))
    fi
done

echo "Total benchmarks to process: ${total_benchmarks}"
echo ""

current=0
for input_file in "${BENCHMARK_PATHS[@]}"; do
    if [ -n "$input_file" ]; then
        ((current++))
        echo ""
        echo "╔══════════════════════════════════════════╗"
        echo "║  Processing benchmark ${current}/${total_benchmarks}"
        echo "╚══════════════════════════════════════════╝"
        echo ""
        process_benchmark "${input_file}"
    fi
done

echo ""
echo "=============================================="
echo "All benchmarks completed successfully!"
echo "=============================================="
echo ""
echo "Summary of results:"
for input_file in "${BENCHMARK_PATHS[@]}"; do
    if [ -n "$input_file" ]; then
        local benchmark_name=$(basename "$(dirname "$input_file")")
        local input_dir=$(dirname "${input_file}")
        local metrics_file="${input_dir}/output_metrics.txt"
        
        if [ -f "${metrics_file}" ]; then
            echo "  - ${benchmark_name}:"
            cat "${metrics_file}" | sed 's/^/      /'
        fi
    fi
done
echo ""
